# Lab1
isArmstrong <- function(x) {
len <- nchar(as.character(x))
sum <- 0
for (i in 1:len) {
digit <- as.integer(substr(x, i, i))
sum <- sum + digitˆlen
}
return(sum == x)
}
# Test the function with some examples
isArmstrong(153) # TRUE

set.seed(1143)
X<- rpois(10, 10)
Y <- rnorm(10)
sort(X)
order(X)

RegisterNos <- c(1, 2, 3, 4, 5)
StuNames <- c('Raj', 'Ram', 'Ramesh', 'Rajesh', 'Suresh')
StuSubjs <- list(c('EDA', 'Data Vis'), c('AI', 'ML'), c('English', 'Psychology'), c('Test'), c('AnthropoStuYr <- c(2020, 2020, 2021, 2021, 2022)
StuNoD <- c('SCOPE', 'SCOPE', 'Arts', 'Secret', 'Community')
StuInsti <- c('VIT', 'VIT', 'Symbiosis', 'FBI', 'Greendale')
notadataframe <- list(RegisterNos, StuNames, StuSubjs, StuYr, StuNoD, StuInsti)
notadataframe


Name <- c("Aby", "Arya", "Ash", "Adhi")
Age <- c(20, 19, 19, 20)
Number <- c("18MIS2022", "18MIS2012", "18MIS0022", "18MIS0002")
student <- data.frame(Name, Age, Number)

library(dplyr)
# Group the data by department and gender
data_by_department_gender <- company %>%
group_by(Dept, Gender)
# Calculate the total salary for each group
data_by_department_gender <- data_by_department_gender %>%
summarize(Total_Salary = sum(payroll))
# Print the resulting dataframe
data_by_department_gender

company %>%
filter(payroll==max(payroll)) %>%
select(Name, Dept)

pain %>%
filter(Club=="Football", Age>=17) %>%
select(Fname, Location)

pain %>%
select(Club, Level) %>%
group_by(Club, Level) %>%
summarise(count=n())

pain %>%
select(Fname, DateOfJoining) %>%
filter(Gender=="F")

pain %>%
select(DateOfJoining) %>%
filter(grepl("ˆB", Education))

Lab 2

#for dependent var
outliers_price <- boxplot.stats(df_outputs$price)$out
length(outliers_price)
my3<-df[, c(1, 5, 6, 8, 9, 10)]
#for independent var
for(x in 1:ncol(df_inputs))
{
  my1<-colnames(df_inputs)
  f<-sapply(df_inputs, is.numeric)
  if(!f[x])
  {
    print(paste(my1[x], "is a factor"))
  }
}
outliers <- boxplot.stats(df_inputs$carat)$out
length(outliers) ???

ggplot(data=df, aes(x=cut, color=cut))+geom_histogram(stat="count")
ggplot(data=df, aes(x=price, y=carat))+geom_point(aes(color=cut), alpha=1/10)+geom_smooth()
ggplot(data=df, aes(x=price, color=carat))+geom_histogram(stat="count")
ggplot(data=df, aes(x=color, fill=color))+geom_bar()
ggplot(data=df, aes(x=clarity, fill=clarity))+geom_bar()

IQR(df$price) # interquartilerange

cormat<-round(cor(my3), 2)
library(reshape2)
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + geom_tile()

lab 3

fit <- lm(y~x)
print(coef(fit))


library(ggplot2)
df <- data.frame(x, y)
# Generate the plot
ggplot(df, aes(x, y)) +
geom_point(color="purple", size=4) +
geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], color = "chartreuse") +
ggtitle("Linear Regression of y on x") +
xlab("x") +
ylab("y")


residuals <- residuals(fit)
rs <- (residualsˆ2)
# RSS is a single number, there is no minimum
# Finding minimum of the residuals squared and RSS
print(min(rs))


residuals_df <- data.frame(x, residuals)
ggplot(residuals_df, aes(x, residuals)) +
geom_point(color="red") +
ggtitle("Residuals plot") +
xlab("x") +
ylab("Residuals")


se_coef <- summary(fit)$coef[,2]
print(se_coef)

confint <- confint(fit, level = 0.95)
print(confint)

r_squared <- summary(fit)$r.squared
print(r_squared)

df$predictions <- predict(fit, newdata=df)
print(df)

scor <- cor(swiss)
corrplot(scor, order="hclust", method="color", type="lower",
title="\n\nCorrelation matrix", tl.srt=30)

fit1 <- lm(Examination ~ Fertility + Agriculture + Catholic + Education)

# Summarize the model fit
summary(fit1)

Lab 4

boxplot(data)

aovFirst <- aov(First ~ Second+Third, data=data)
summary(aovFirst)

data <- PlantGrowth
boxplot(data$weight ~ data$group)

aov.plants <- aov(weight ~ group, data=data)
summary(aov.plants)

library(granovaGG)
data("poison")
model <- aov(RateSurvTime ~ Treatment, data = poison)
summary(model)
boxplot(RateSurvTime ~ Treatment, data = poison, xlab = "Treatment", ylab = "RateSurvTime")

lab 5

library(lattice)
xyplot(psavert + unemploy/pop*100 ~ date,
main="USA economics raw plot",
key = list(space="right",
lines=list(col=c("blue","magenta"), lty=c(2,2),
lwd=6),
text=list(c("Personal Savings Rate", "Unemployment
Rate"))),
xlab="Date",
ylab="Personal Savings Rate (blue)\n Unemployment Rate per
million (pink)")

library(TTR)
library(forecast)
library(lubridate)

df$date <- ymd(df$date) # Convert Date column to date format
# Simple Moving Average
df$sma <- SMA(df$psavert, n = 5)
# Weighted Moving Average
df$wma <- WMA(df$psavert, n=5, wts=c(3, 2, 1, 2, 3))
# Exponential Smoothing
ts_psavert <- ts(df$psavert, start=c(1967, 01), frequency=12)
exp_smooth1 <- HoltWinters(ts_psavert, alpha=0.2, beta=FALSE)
# ARIMA
arima1 <- auto.arima(ts_psavert)
library(graphics)
plot(exp_smooth1$fitted[,1], xlab="Years", ylab="Personal Savings Rate", main="HoltWinters Chart", col=lines(ts_psavert, col="black")

plot(forecast(arima1, h=20), xlab="Years", ylab="Personal Savings Rate", main="ARIMA Chart", col="red", lines(ts_psavert, col="green")

ggplot(df, aes(x=date)) +
geom_line(aes(y=psavert, color="psavert")) +
geom_line(aes(y=sma, color="SMA"), linetype="dashed") +
geom_line(aes(y=wma, color="WMA"), linetype="dotted") +
#geom_line(aes(y=exp_smooth, color="Exp. Smooth")) +
ggtitle("Personal Savings Rate Moving Averages") +
scale_color_discrete(name="Line Type", labels=c("Personal Savings Rate", "SMA", "WMA")) + #, "Exp. Smxlab("Year") +
ylab("Rate")

# get the forecast
fcst1 <- forecast(exp_smooth1)
# mean absolute error
exp_smooth_mae <- mean(abs(fcst1$residuals), na.rm = TRUE)
# mean squared error
exp_smooth_mse <- mean(fcst1$residualsˆ2, na.rm = TRUE)
# root of mean squared error
exp_smooth_rmse <- sqrt(exp_smooth_mse)
# mean absolute percentage error
exp_smooth_mape <- mean(abs(fcst1$residuals/df$psavert), na.rm = TRUE)
fcst2 <- forecast(arima1)
arima_mae <- mean(abs(fcst2$residuals), na.rm = TRUE)
arima_mse <- mean(fcst2$residualsˆ2, na.rm = TRUE)
arima_rmse <- sqrt(exp_smooth_mse)
arima_mape <- mean(abs(fcst2$residuals/df$psavert), na.rm = TRUE)

MSE_sma <- mean((df$psavert - df$sma)ˆ2, na.rm = TRUE)
RMSE_sma <- sqrt(MSE_sma)
sma_mape <- mean(abs((df$psavert - df$sma) / df$psavert), na.rm = TRUE) * 100

library(knitr)
accuracy_metrics <- data.frame(Model = c("Simple Moving Average", "Weighted Moving Average", "ExponentiaMAPE =
c(sma_mape, wma_mape, exp_smooth_mape, arima_mape),
MSE = c(MSE_sma, MSE_wma, exp_smooth_mse, arima_mse),
RMSE = c(RMSE_sma, RMSE_wma, exp_smooth_rmse, arima_rmse))
kable(accuracy_metrics, caption = "Accuracy Metrics for Time Series Models")

lab 6

# First, separate the target variable and numeric feature variables
data.class <- data$diagnosis
data.class <- as.factor(data.class)
data.numeric <- data[sapply(data, is.numeric)]
data.values <- data.numeric[!names(data.numeric) %in% c("id")]

# Create a sample of indices to split the data into train and test sets
indexes <- sample(1:nrow(data.values), 0.7 * nrow(data.values))
# Split the data into training and test sets
dataX.train <- data.values[indexes, ]
dataX.test <- data.values[-indexes, ]
dataY.train <- data.class[indexes]
dataY.test <- data.class[-indexes]

# Logistic Regression Model
model_logit <- glm(dataY.train ~ ., data=dataX.train, family="binomial")

# predictions are numerical (regression) confert to factors
predictions_logit <- as.factor(
ifelse(
predict(model_logit,
newdata = dataX.test,
type = "response") > 0.5, "M", "B"))
# library for confusion matrix
library(caret)
csm <- confusionMatrix(predictions_logit, dataY.test)
print(csm)

lab 7

library(e1071)
library(dplyr)

iris_summary <- iris %>%
group_by(Species) %>%
summarise(
count = n(),
mean_sepal_length = mean(Sepal.Length),
mean_sepal_width = mean(Sepal.Width),
mean_petal_length = mean(Petal.Length),
mean_petal_width = mean(Petal.Width),
sd_sepal_length = sd(Sepal.Length),
sd_sepal_width = sd(Sepal.Width),
sd_petal_length = sd(Petal.Length),
sd_petal_width = sd(Petal.Width)
)
print(iris_summary)

set.seed(1143)
train <- sample(nrow(iris), nrow(iris)*0.7)
train_data <- iris[train, ]
test_data <- iris[-train, ]
nb_model <- naiveBayes(Species ~ ., data=train_data)
nb_train_pred <- predict(nb_model, train_data)
nb_test_pred <- predict(nb_model, test_data)

nb_results_df <- data.frame(
dataset = c(rep("training", length(nb_train_pred)), rep("testing", length(nb_test_pred))),
predicted = c(nb_train_pred, nb_test_pred),
actual = c(train_data$Species, test_data$Species)
)

library(ggplot2)
ggplot(nb_results_df, aes(x = dataset, fill = predicted == actual)) +
geom_bar(position = "fill") +
scale_fill_manual(values = c("darkorange", "steelblue")) +
ggtitle("Naive Bayes Classification Results - Accuracy") +
ylab("Proportion") +
xlab("Dataset")

bigg <- ggplot(test_data, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
geom_point() +
ggtitle("Test Data - Scatter Plot") +
xlab("Sepal Length") +
ylab("Sepal Width")
nb_g <- ggplot(test_data, aes(x = Sepal.Length, y = Sepal.Width, color = nb_test_pred)) +
geom_point() +
ggtitle("NaiveBayes Predictions") +
xlab("Sepal Length") +
ylab("Sepal Width")

library(caret)
scores_df <- data.frame(model=character(), Accuracy=numeric(), Kappa=numeric(), stringsAsFactors = FALSEnb_confmat <- confusionMatrix(nb_test_pred, test_data$Species)
new_row <- data.frame(model = "Naive Bayes",
Accuracy = nb_confmat$overall["Accuracy"],
Kappa = nb_confmat$overall["Kappa"])
scores_df <- rbind(scores_df, new_row)
print(nb_confmat)

library(randomForest)
rf_model <- randomForest(Species ~., data=train_data)
rf_train_pred <- predict(rf_model, train_data)
rf_test_pred <- predict(rf_model, test_data)

rf_results_df <- data.frame(
dataset = c(rep("training", length(rf_train_pred)), rep("testing", length(rf_test_pred))),
predicted = c(rf_train_pred, rf_test_pred),
actual = c(train_data$Species, test_data$Species)
)

library(ggplot2)
ggplot(rf_results_df, aes(x = dataset, fill = predicted == actual)) +
geom_bar(position = "fill") +
scale_fill_manual(values = c("darkorange", "steelblue")) +
ggtitle("RandomForest Results - Accuracy") +
ylab("Proportion") +
xlab("Dataset")

rf_g <- ggplot(test_data, aes(x = Sepal.Length, y = Sepal.Width, color = rf_test_pred)) +
geom_point() +
ggtitle("Random Forest Predictions") +
xlab("Sepal Length") +
ylab("Sepal Width")

rf_confmat <- confusionMatrix(rf_test_pred, test_data$Species)
new_row <- data.frame(model = "Random Forest",
Accuracy = rf_confmat$overall["Accuracy"],
Kappa = rf_confmat$overall["Kappa"])
scores_df <- rbind(scores_df, new_row)
print(rf_confmat)

#it is under e1071 package
svm_model <- svm(Species ~., data=train_data)
svm_train_pred <- predict(svm_model, train_data)
svm_test_pred <- predict(svm_model, test_data)

svm_results_df <- data.frame(
dataset = c(rep("training", length(svm_train_pred)), rep("testing", length(svm_test_pred))),
predicted = c(svm_train_pred, svm_test_pred),
actual = c(train_data$Species, test_data$Species)
)

library(ggplot2)
ggplot(svm_results_df, aes(x = dataset, fill = predicted == actual)) +
geom_bar(position = "fill") +
scale_fill_manual(values = c("darkorange", "steelblue")) +
ggtitle("Support Vector Machine - Accuracy") +
ylab("Proportion") +
xlab("Dataset")

svm_g <- ggplot(test_data, aes(x = Sepal.Length, y = Sepal.Width, color = svm_test_pred)) +
geom_point() +
ggtitle("SVM Prediction") +
xlab("Sepal Length") +
ylab("Sepal Width")
svm_confmat <- confusionMatrix(svm_test_pred, test_data$Species)
new_row <- data.frame(model = "SVM",
Accuracy = svm_confmat$overall["Accuracy"],
Kappa = svm_confmat$overall["Kappa"])
scores_df <- rbind(scores_df, new_row)
print(svm_confmat)


library(gridExtra)
grid.arrange(bigg, nb_g, rf_g, svm_g, nrow = 2, ncol = 2)
print(scores_df)

lab 8
# set the working directory to the folder containing the CSV file
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# read the CSV file into R
wine <- read.csv("wineQualityN.csv")
wine <- na.omit(wine)
# show the first 5 rows of the data
head(wine, 5)

library(cluster)
scores <- data.frame("Method" = NULL, "Silhouette" = NULL)

# Perform k-means clustering with k=2
set.seed(1143)
k <- 2
# wine[0] is the class labels of Red, White, and Blue
kmeans_fit <- kmeans(wine[,2:13], k)
# Compute the within-cluster sum of squares (WSS)
# wss <- sum(kmeans_fit$withinss)
# Compute the between-cluster sum of sqaures (BSS)
# bss <- kmeans_fit$betweenss # total sum of squares - within ss
# Compute the silhouette score for each point
sil_width <- silhouette(kmeans_fit$cluster, dist(wine[,2:13]))
# Compute the average silhouette score for the whole clustering
sil <- mean(sil_width[,3])
# Plot the clustering results
clusplot(wine[,2:13], kmeans_fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0, main="Cluster Plot")
new_row = data.frame("Method"="K-Means", "Silhouette"=sil)
scores <- rbind(scores, new_row)

# apply k-medoids clustering with k=2
kmed <- pam(wine[2:13], k=2, diss=TRUE)
## Warning in as.dist.default(x): non-square matrix
# Compute the silhouette score for each point
sil_width <- silhouette(kmeans_fit$cluster, dist(wine[,2:13]))
# Compute the average silhouette score for the whole clustering
sil <- mean(sil_width[,3])
# Plot the clustering results
clusplot(wine[,2:13], kmed$cluster, color=TRUE, shade=TRUE, labels=2, lines=0, main="Cluster Plot for K-Medoits")
new_row = data.frame("Method"="K-Medoids", "Silhouette"=sil)
scores <- rbind(scores, new_row)

wine_sub <- wine[, 2:13]
# Perform hierarchical clustering
hc <- hclust(dist(wine_sub), method = "ward.D2")
plot(hc) 

# Cut the dendrogram to get 2 clusters
clusters <- cutree(hc, k = 2)
# Get the silhouette score for 2 clusters
sil_width <- silhouette(clusters, dist(wine_sub))
sil <- mean(sil_width[,3])
new_row = data.frame("Method"="Hierarchial", "Silhouette"=sil)
scores <- rbind(scores, new_row)
