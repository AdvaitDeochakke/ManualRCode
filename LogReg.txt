# Define the sigmoid function
sigmoid <- function(z) {
  return(1 / (1 + exp(-z)))
}

# Define the cost function
cost <- function(X, y, theta) {
  m <- length(y)
  h <- sigmoid(X %*% theta)
  J <- (1 / m) * (-t(y) %*% log(h) - t(1 - y) %*% log(1 - h))
  return(J)
}

# Define the gradient function
gradient <- function(X, y, theta) {
  m <- length(y)
  h <- sigmoid(X %*% theta)
  grad <- (1 / m) * t(X) %*% (h - y)
  return(grad)
}

# Define the logistic regression function
logistic_regression <- function(X, y, alpha, num_iters) {
  m <- nrow(X)
  n <- ncol(X)
  theta <- matrix(0, nrow = n, ncol = 1)
  J_history <- numeric()
  
  for (i in 1:num_iters) {
    theta <- theta - alpha * gradient(X, y, theta)
    J_history <- c(J_history, cost(X, y, theta))
  }
  
  return(list(theta = theta, J_history = J_history))
}

# Generate a random dataset
set.seed(0)
m <- 1000
n <- 2
X <- matrix(rnorm(m * n), nrow = m, ncol = n)
y <- as.integer(runif(m) < sigmoid(X %*% matrix(c(1, 2), nrow = 2) + 0.5))

# Add intercept term to X
X <- cbind(1, X)

# Initialize hyperparameters
alpha <- 0.01
num_iters <- 1000

# Perform logistic regression
result <- logistic_regression(X, y, alpha, num_iters)

# Plot the cost history
plot(result$J_history, type = "l", xlab = "Iteration", ylab = "Cost")
